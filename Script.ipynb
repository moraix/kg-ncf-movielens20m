{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5a16f9a",
   "metadata": {},
   "source": [
    "# Enhancing Movie Recommendations with Knowledge Graph Embeddings and Neural Collaborative Filtering\n",
    "\n",
    "This notebook builds a Knowledge Graph from IMDb TSV files stored in Google Drive.\n",
    "\n",
    "**Entities:**\n",
    "- Movies (tconst)\n",
    "- Persons (nconst)\n",
    "- Genres (from title.basics genres column)\n",
    "\n",
    "**Relations:**\n",
    "- (movie) --HAS_GENRE--> (genre)\n",
    "- (person) --DIRECTED--> (movie)\n",
    "- (person) --WROTE--> (movie)\n",
    "- (person) --ACTED_IN--> (movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420689ba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages (if not already installed)\n",
    "!pip install pandas tqdm -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c7ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "DRIVE_ROOT = '/content/drive/MyDrive'\n",
    "INPUT_DIR = f'{DRIVE_ROOT}/imdb_data'  # Folder containing TSV files\n",
    "OUTPUT_DIR = f'{DRIVE_ROOT}/kg_output'  # Output folder for triples and maps\n",
    "\n",
    "# Dry-run mode: set to None for full processing, or an integer to limit rows per file\n",
    "LIMIT_ROWS = None  # Change to 10000 for quick testing\n",
    "\n",
    "# File names\n",
    "FILES = {\n",
    "    'basics': 'title.basics.tsv',\n",
    "    'crew': 'title.crew.tsv',\n",
    "    'principals': 'title.principals.tsv',\n",
    "    'ratings': 'title.ratings.tsv'  # Not used for triples but can be read\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Input directory: {INPUT_DIR}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Limit rows: {LIMIT_ROWS if LIMIT_ROWS else 'None (full processing)'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c811a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Output directory ready: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a966f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def safe_split(value, sep=','):\n",
    "    \"\"\"Safely split a value, handling missing values marked as \\\\N\"\"\"\n",
    "    if pd.isna(value) or value == '\\\\N' or value == '':\n",
    "        return []\n",
    "    return [v.strip() for v in str(value).split(sep) if v.strip() and v.strip() != '\\\\N']\n",
    "\n",
    "def read_tsv_chunked(filepath, limit=None, chunksize=100000):\n",
    "    \"\"\"Read TSV file with optional row limit and chunking\"\"\"\n",
    "    if limit:\n",
    "        # For dry-run, read only first N rows\n",
    "        df = pd.read_csv(filepath, sep='\\t', nrows=limit, low_memory=False)\n",
    "        return [df]  # Return as single chunk\n",
    "    else:\n",
    "        # Full processing with chunking\n",
    "        return pd.read_csv(filepath, sep='\\t', chunksize=chunksize, low_memory=False)\n",
    "\n",
    "def get_file_path(filename):\n",
    "    \"\"\"Get full path for a file in INPUT_DIR\"\"\"\n",
    "    return os.path.join(INPUT_DIR, filename)\n",
    "\n",
    "print(\"Helper functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96c33df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract entities and build triples from title.basics.tsv\n",
    "# Entities: movies (tconst), genres\n",
    "# Relations: (movie) --HAS_GENRE--> (genre)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Step 1: Processing title.basics.tsv\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "triples = []\n",
    "movies = set()\n",
    "genres = set()\n",
    "\n",
    "basics_path = get_file_path(FILES['basics'])\n",
    "print(f\"Reading: {basics_path}\")\n",
    "\n",
    "chunks = read_tsv_chunked(basics_path, limit=LIMIT_ROWS)\n",
    "total_chunks = len(chunks) if isinstance(chunks, list) else None\n",
    "\n",
    "for chunk_idx, chunk in enumerate(chunks):\n",
    "    print(f\"Processing chunk {chunk_idx + 1}... (rows: {len(chunk)})\")\n",
    "    \n",
    "    # Filter out rows with missing tconst\n",
    "    chunk = chunk[chunk['tconst'].notna() & (chunk['tconst'] != '\\\\N')]\n",
    "    \n",
    "    for _, row in tqdm(chunk.iterrows(), total=len(chunk), desc=f\"Chunk {chunk_idx + 1}\"):\n",
    "        movie_id = str(row['tconst']).strip()\n",
    "        if not movie_id or movie_id == '\\\\N':\n",
    "            continue\n",
    "            \n",
    "        movies.add(movie_id)\n",
    "        \n",
    "        # Extract genres\n",
    "        genre_list = safe_split(row.get('genres', ''))\n",
    "        for genre in genre_list:\n",
    "            if genre:\n",
    "                genres.add(genre)\n",
    "                triples.append(('HAS_GENRE', movie_id, genre))\n",
    "\n",
    "print(f\"\\nStep 1 Complete:\")\n",
    "print(f\"  Movies found: {len(movies)}\")\n",
    "print(f\"  Genres found: {len(genres)}\")\n",
    "print(f\"  HAS_GENRE triples: {sum(1 for t in triples if t[0] == 'HAS_GENRE')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3e76ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Extract relations from title.crew.tsv\n",
    "# Relations: (person) --DIRECTED--> (movie), (person) --WROTE--> (movie)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Step 2: Processing title.crew.tsv\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "persons = set()\n",
    "crew_path = get_file_path(FILES['crew'])\n",
    "print(f\"Reading: {crew_path}\")\n",
    "\n",
    "chunks = read_tsv_chunked(crew_path, limit=LIMIT_ROWS)\n",
    "directed_count = 0\n",
    "wrote_count = 0\n",
    "\n",
    "for chunk_idx, chunk in enumerate(chunks):\n",
    "    print(f\"Processing chunk {chunk_idx + 1}... (rows: {len(chunk)})\")\n",
    "    \n",
    "    # Filter out rows with missing tconst\n",
    "    chunk = chunk[chunk['tconst'].notna() & (chunk['tconst'] != '\\\\N')]\n",
    "    \n",
    "    for _, row in tqdm(chunk.iterrows(), total=len(chunk), desc=f\"Chunk {chunk_idx + 1}\"):\n",
    "        movie_id = str(row['tconst']).strip()\n",
    "        if not movie_id or movie_id == '\\\\N':\n",
    "            continue\n",
    "        \n",
    "        # Extract directors\n",
    "        directors = safe_split(row.get('directors', ''))\n",
    "        for director_id in directors:\n",
    "            if director_id:\n",
    "                persons.add(director_id)\n",
    "                triples.append(('DIRECTED', director_id, movie_id))\n",
    "                directed_count += 1\n",
    "        \n",
    "        # Extract writers\n",
    "        writers = safe_split(row.get('writers', ''))\n",
    "        for writer_id in writers:\n",
    "            if writer_id:\n",
    "                persons.add(writer_id)\n",
    "                triples.append(('WROTE', writer_id, movie_id))\n",
    "                wrote_count += 1\n",
    "\n",
    "print(f\"\\nStep 2 Complete:\")\n",
    "print(f\"  Persons found so far: {len(persons)}\")\n",
    "print(f\"  DIRECTED triples: {directed_count}\")\n",
    "print(f\"  WROTE triples: {wrote_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e31d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Extract relations from title.principals.tsv\n",
    "# Relations: (person) --ACTED_IN--> (movie) where category in {\"actor\", \"actress\"}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Step 3: Processing title.principals.tsv\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "principals_path = get_file_path(FILES['principals'])\n",
    "print(f\"Reading: {principals_path}\")\n",
    "\n",
    "chunks = read_tsv_chunked(principals_path, limit=LIMIT_ROWS)\n",
    "acted_count = 0\n",
    "\n",
    "# Valid acting categories\n",
    "ACTING_CATEGORIES = {'actor', 'actress'}\n",
    "\n",
    "for chunk_idx, chunk in enumerate(chunks):\n",
    "    print(f\"Processing chunk {chunk_idx + 1}... (rows: {len(chunk)})\")\n",
    "    \n",
    "    # Filter for acting roles only\n",
    "    chunk = chunk[\n",
    "        chunk['tconst'].notna() & \n",
    "        (chunk['tconst'] != '\\\\N') &\n",
    "        chunk['nconst'].notna() & \n",
    "        (chunk['nconst'] != '\\\\N') &\n",
    "        chunk['category'].notna()\n",
    "    ]\n",
    "    \n",
    "    # Filter by category\n",
    "    chunk = chunk[chunk['category'].str.lower().isin(ACTING_CATEGORIES)]\n",
    "    \n",
    "    for _, row in tqdm(chunk.iterrows(), total=len(chunk), desc=f\"Chunk {chunk_idx + 1}\"):\n",
    "        movie_id = str(row['tconst']).strip()\n",
    "        person_id = str(row['nconst']).strip()\n",
    "        \n",
    "        if movie_id and person_id and movie_id != '\\\\N' and person_id != '\\\\N':\n",
    "            persons.add(person_id)\n",
    "            triples.append(('ACTED_IN', person_id, movie_id))\n",
    "            acted_count += 1\n",
    "\n",
    "print(f\"\\nStep 3 Complete:\")\n",
    "print(f\"  Total persons: {len(persons)}\")\n",
    "print(f\"  ACTED_IN triples: {acted_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64481b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create entity and relation mappings\n",
    "# Map string entities/relations to integer IDs\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Step 4: Creating entity and relation mappings\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Collect all entities\n",
    "all_entities = set()\n",
    "all_entities.update(movies)\n",
    "all_entities.update(persons)\n",
    "all_entities.update(genres)\n",
    "\n",
    "# Create entity mapping (string -> integer ID)\n",
    "entity_map = {entity: idx for idx, entity in enumerate(sorted(all_entities), start=1)}\n",
    "# Reserve 0 for padding if needed (optional)\n",
    "\n",
    "# Create relation mapping\n",
    "relations = {'HAS_GENRE', 'DIRECTED', 'WROTE', 'ACTED_IN'}\n",
    "relation_map = {rel: idx for idx, rel in enumerate(sorted(relations), start=1)}\n",
    "\n",
    "print(f\"Entity mapping created: {len(entity_map)} entities\")\n",
    "print(f\"Relation mapping created: {len(relation_map)} relations\")\n",
    "print(f\"\\nRelations: {sorted(relations)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef93178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Convert triples to integer IDs and create final dataset\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Step 5: Converting triples to integer IDs\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Filter triples to only include entities that exist in our entity map\n",
    "# (should be all, but safety check)\n",
    "valid_triples = []\n",
    "missing_entities = set()\n",
    "\n",
    "for rel, head, tail in tqdm(triples, desc=\"Converting triples\"):\n",
    "    if head in entity_map and tail in entity_map and rel in relation_map:\n",
    "        valid_triples.append({\n",
    "            'head': entity_map[head],\n",
    "            'relation': relation_map[rel],\n",
    "            'tail': entity_map[tail],\n",
    "            'head_str': head,\n",
    "            'relation_str': rel,\n",
    "            'tail_str': tail\n",
    "        })\n",
    "    else:\n",
    "        if head not in entity_map:\n",
    "            missing_entities.add(head)\n",
    "        if tail not in entity_map:\n",
    "            missing_entities.add(tail)\n",
    "\n",
    "if missing_entities:\n",
    "    print(f\"Warning: {len(missing_entities)} entities not found in mapping (should not happen)\")\n",
    "\n",
    "print(f\"Valid triples: {len(valid_triples)}\")\n",
    "print(f\"Original triples: {len(triples)}\")\n",
    "print(f\"Filtered out: {len(triples) - len(valid_triples)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29393545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Save outputs to Google Drive\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Step 6: Saving outputs\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save triples.csv (with integer IDs)\n",
    "triples_df = pd.DataFrame(valid_triples)\n",
    "triples_df[['head', 'relation', 'tail']].to_csv(\n",
    "    os.path.join(OUTPUT_DIR, 'triples.csv'),\n",
    "    index=False\n",
    ")\n",
    "print(f\"Saved: {OUTPUT_DIR}/triples.csv\")\n",
    "print(f\"  Shape: {triples_df.shape}\")\n",
    "\n",
    "# Save entity_map.csv\n",
    "entity_map_df = pd.DataFrame([\n",
    "    {'entity_id': idx, 'entity': entity}\n",
    "    for entity, idx in sorted(entity_map.items(), key=lambda x: x[1])\n",
    "])\n",
    "entity_map_df.to_csv(\n",
    "    os.path.join(OUTPUT_DIR, 'entity_map.csv'),\n",
    "    index=False\n",
    ")\n",
    "print(f\"Saved: {OUTPUT_DIR}/entity_map.csv\")\n",
    "print(f\"  Shape: {entity_map_df.shape}\")\n",
    "\n",
    "# Save relation_map.csv\n",
    "relation_map_df = pd.DataFrame([\n",
    "    {'relation_id': idx, 'relation': rel}\n",
    "    for rel, idx in sorted(relation_map.items(), key=lambda x: x[1])\n",
    "])\n",
    "relation_map_df.to_csv(\n",
    "    os.path.join(OUTPUT_DIR, 'relation_map.csv'),\n",
    "    index=False\n",
    ")\n",
    "print(f\"Saved: {OUTPUT_DIR}/relation_map.csv\")\n",
    "print(f\"  Shape: {relation_map_df.shape}\")\n",
    "\n",
    "print(\"\\nAll outputs saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a94b7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Print statistics\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"KNOWLEDGE GRAPH STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nEntities:\")\n",
    "print(f\"  Movies: {len(movies)}\")\n",
    "print(f\"  Persons: {len(persons)}\")\n",
    "print(f\"  Genres: {len(genres)}\")\n",
    "print(f\"  Total entities: {len(all_entities)}\")\n",
    "\n",
    "print(f\"\\nRelations:\")\n",
    "for rel in sorted(relations):\n",
    "    count = sum(1 for t in valid_triples if t['relation_str'] == rel)\n",
    "    print(f\"  {rel}: {count:,} triples\")\n",
    "\n",
    "print(f\"\\nTotal triples: {len(valid_triples):,}\")\n",
    "\n",
    "print(f\"\\nOutput files:\")\n",
    "print(f\"  {OUTPUT_DIR}/triples.csv\")\n",
    "print(f\"  {OUTPUT_DIR}/entity_map.csv\")\n",
    "print(f\"  {OUTPUT_DIR}/relation_map.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Pipeline complete!\")\n",
    "print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
